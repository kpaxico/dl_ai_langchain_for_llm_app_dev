{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "073bf8f9",
   "metadata": {},
   "source": [
    "# LangChain: Models, Prompts and Output Parsers\n",
    "\n",
    "\n",
    "## Outline\n",
    "\n",
    " * Direct API calls to OpenAI\n",
    " * API calls through LangChain:\n",
    "   * Prompts\n",
    "   * Models\n",
    "   * Output parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01ff606",
   "metadata": {},
   "source": [
    "## Get your [OpenAI API Key](https://platform.openai.com/account/api-keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b426111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this command in the terminal to install the required packages:\n",
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b7ed03ed-1322-49e3-b2a2-33e94fb592ef",
   "metadata": {
    "height": 115,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# read local .env file\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# Set the model to use\n",
    "llm_model = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbad9cdb",
   "metadata": {},
   "source": [
    "## Chat API : OpenAI\n",
    "\n",
    "Let's start with a direct API calls to OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "484bfa6a",
   "metadata": {
    "height": 166,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=llm_model):\n",
    "    response = client.responses.create(\n",
    "        model=model,\n",
    "        input=prompt,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.output_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a1d076ce",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 + 1 equals 2.'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1b32b57a",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks\n",
      "into a style that is American English in a calm and respectful tone\n",
      ".\n",
      "text: ```\n",
      "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse,the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse,\\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\"\n",
    "\n",
    "style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"Translate the text \\\n",
    "that is delimited by triple backticks\n",
    "into a style that is {style}.\n",
    "text: ```{customer_email}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c883dcbd",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm quite upset that my blender lid came off and splattered smoothie all over my kitchen walls. To make matters worse, the warranty doesn't cover the cost of cleaning up. I could really use your help right now.\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = get_completion(prompt)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80482d1",
   "metadata": {},
   "source": [
    "## Chat API : LangChain\n",
    "\n",
    "Let's try how we can do the same using LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25c5b27",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f0d4a269",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1cc0c8b8",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001F70601E560>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001F70601C6D0>, root_client=<openai.OpenAI object at 0x000001F705AD7C70>, root_async_client=<openai.AsyncOpenAI object at 0x000001F70601E3E0>, model_name='gpt-4o', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To control the randomness and creativity of the generated\n",
    "# text by an LLM, use temperature = 0.0\n",
    "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d07256",
   "metadata": {},
   "source": [
    "### Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "57bda7d8",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [],
   "source": [
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3a31f246",
   "metadata": {
    "height": 96,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create a prompt template from a template string\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cac2cb16",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['style', 'text'], input_types={}, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the prompt template\n",
    "prompt_template.messages[0].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cdc5566c",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['style', 'text']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the input variables of the prompt template\n",
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba77cf4",
   "metadata": {},
   "source": [
    "`format_messages` is a **batch operation** that:\n",
    "1. Takes variable values as input\n",
    "2. Finds **ALL** messages in the prompt template that contain those variables\n",
    "3. Replaces the variables in ALL those messages with the same values\n",
    "4. Returns a list of ALL formatted messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bbd51a93",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer_style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\"\n",
    "\n",
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\"\n",
    "\n",
    "# Format the messages using the template\n",
    "customer_messages = prompt_template.format_messages(\n",
    "                    style=customer_style, # Applied to ALL messages containing {style}\n",
    "                    text=customer_email)  # Applied to ALL messages containing {text}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd97021",
   "metadata": {},
   "source": [
    "**LangChain Message Types:**\n",
    "LangChain has different message types for different roles:\n",
    "\n",
    "1. **HumanMessage**: Messages from humans/users\n",
    "2. **AIMessage**: Messages from AI assistants\n",
    "3. **SystemMessage**: System instructions\n",
    "4. **ChatMessage**: Generic messages with custom roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8c09d8b4",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'langchain_core.messages.human.HumanMessage'>\n"
     ]
    }
   ],
   "source": [
    "print(type(customer_messages))\n",
    "print(type(customer_messages[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e02dafa2",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone\\n. text: ```\\nArrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\" additional_kwargs={} response_metadata={}\n"
     ]
    }
   ],
   "source": [
    "print(customer_messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bd789f9f",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Call the LLM to translate to the style of the customer message\n",
    "customer_response = chat.invoke(customer_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ad294407",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm quite upset that my blender lid came off and splattered smoothie all over my kitchen walls. To make matters worse, the warranty doesn't cover the cost of cleaning up my kitchen. I could really use your help right now.\n"
     ]
    }
   ],
   "source": [
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7c267e5f",
   "metadata": {
    "height": 166,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks into a style that is a polite tone that speaks in English Pirate. text: ```Hey there customer, the warranty does not cover cleaning expenses for your kitchen because it's your fault that you misused your blender by forgetting to put the lid on before starting the blender. Tough luck! See ya!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "service_reply = \"\"\"Hey there customer, \\\n",
    "the warranty does not cover \\\n",
    "cleaning expenses for your kitchen \\\n",
    "because it's your fault that \\\n",
    "you misused your blender \\\n",
    "by forgetting to put the lid on before \\\n",
    "starting the blender. \\\n",
    "Tough luck! See ya!\n",
    "\"\"\"\n",
    "\n",
    "service_style_pirate = \"\"\"\\\n",
    "a polite tone \\\n",
    "that speaks in English Pirate\\\n",
    "\"\"\"\n",
    "\n",
    "# Format the messages using the template\n",
    "service_messages = prompt_template.format_messages(\n",
    "    style=service_style_pirate,\n",
    "    text=service_reply)\n",
    "\n",
    "print(service_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a0ae5552",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoy, me hearty customer! I be regretful to inform ye that the warranty be not coverin' the cleanin' expenses for yer galley, as it seems ye may have misused yer blender by forgettin' to secure the lid afore startin' the contraption. Alas, such be the way of the seas! Fair winds to ye!\n"
     ]
    }
   ],
   "source": [
    "service_response = chat.invoke(service_messages)\n",
    "\n",
    "print(service_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36536e79",
   "metadata": {},
   "source": [
    "## Output Parsers\n",
    "\n",
    "Let's start with defining how we would like the LLM output to look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f1ccdff5",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': False, 'delivery_days': 5, 'price_value': 'pretty affordable!'}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"gift\": False,\n",
    "  \"delivery_days\": 5,\n",
    "  \"price_value\": \"pretty affordable!\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "df0f4680",
   "metadata": {
    "height": 540,
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing.  It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product \\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f2386e9c",
   "metadata": {
    "height": 96,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: {text}\\n'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "121bb0d4",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"gift\": true,\n",
      "  \"delivery_days\": 2,\n",
      "  \"price_value\": [\n",
      "    \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Format the messages using the template: replace the {text} variable with the customer_review\n",
    "messages = prompt_template.format_messages(text=customer_review)\n",
    "\n",
    "# Create a chat model with temperature 0.0 and the model set to the llm_model\n",
    "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "\n",
    "# Call the LLM to review the customer's review\n",
    "response = chat.invoke(messages)\n",
    "\n",
    "# Print the response\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "10de1d28",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3a3c0609",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# You will get an error by running this line of code\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# because'gift' is not a dictionary\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 'gift' is a string\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgift\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "# You will get an error by running this line of code\n",
    "# because'gift' is not a dictionary\n",
    "# 'gift' is a string\n",
    "response.content.get('gift')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7de2b8",
   "metadata": {},
   "source": [
    "### Parse the LLM output string into a Python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c2e0ec49",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the JsonOutputParser (more robust than StructuredOutputParser)\n",
    "from langchain_core.output_parsers import JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9dea24b4",
   "metadata": {
    "height": 368,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# With JsonOutputParser, we don't need ResponseSchema objects\n",
    "# Instead, we'll define the expected JSON structure in our prompt\n",
    "# JsonOutputParser is more flexible and robust for handling LLM JSON output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b57e1ba8",
   "metadata": {
    "height": 45,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a JsonOutputParser - no schema needed, it's more flexible\n",
    "output_parser = JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fdeaf4fc",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get format instructions from JsonOutputParser (simpler than StructuredOutputParser)\n",
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1eb176c5",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return a JSON object.\n"
     ]
    }
   ],
   "source": [
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "082947fc",
   "metadata": {
    "height": 385,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a prompt template for JsonOutputParser\n",
    "# We'll be more explicit about the JSON structure we want\n",
    "\n",
    "review_template_2 = \"\"\"\\\n",
    "For the following text, extract the following information and return it as JSON:\n",
    "\n",
    "Extract these fields:\n",
    "- gift: Was the item purchased as a gift for someone else? Answer true if yes, false if not or unknown.\n",
    "- delivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\n",
    "- price_value: Extract any sentences about the value or price, and output them as an array of strings.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "# Create a chat prompt template using the new template\n",
    "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
    "\n",
    "# Format the messages using the template\n",
    "messages = prompt.format_messages(text=customer_review, format_instructions=format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1f1537a7",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the following text, extract the following information and return it as JSON:\n",
      "\n",
      "Extract these fields:\n",
      "- gift: Was the item purchased as a gift for someone else? Answer true if yes, false if not or unknown.\n",
      "- delivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\n",
      "- price_value: Extract any sentences about the value or price, and output them as an array of strings.\n",
      "\n",
      "text: This leaf blower is pretty amazing.  It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
      "\n",
      "\n",
      "Return a JSON object.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7b663657",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8312f027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw response content:\n",
      "'```json\\n{\\n  \"gift\": true,\\n  \"delivery_days\": 2,\\n  \"price_value\": [\\n    \"It\\'s slightly more expensive than the other leaf blowers out there, but I think it\\'s worth it for the extra features.\"\\n  ]\\n}\\n```'\n",
      "\n",
      "==================================================\n",
      "\n",
      "Formatted response content:\n",
      "```json\n",
      "{\n",
      "  \"gift\": true,\n",
      "  \"delivery_days\": 2,\n",
      "  \"price_value\": [\n",
      "    \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "==================================================\n",
      "\n",
      "JSON parsed response content:\n",
      "‚úÖ Successfully parsed with JsonOutputParser!\n",
      "Parsed data:\n",
      "{'gift': True, 'delivery_days': 2, 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]}\n"
     ]
    }
   ],
   "source": [
    "# Debug: Let's see the raw response content first\n",
    "print(\"Raw response content:\")\n",
    "print(repr(response.content))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Formatted response content:\")\n",
    "print(response.content)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Use JsonOutputParser - it's much more robust than StructuredOutputParser\n",
    "try:\n",
    "    print(\"JSON parsed response content:\")\n",
    "    output_dict = output_parser.parse(response.content)\n",
    "    print(\"‚úÖ Successfully parsed with JsonOutputParser!\")\n",
    "    print(\"Parsed data:\")\n",
    "    print(output_dict)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå JsonOutputParser failed with error: {e}\")\n",
    "    print(\"Raw response content:\")\n",
    "    print(repr(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d48b647a",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': True,\n",
       " 'delivery_days': 2,\n",
       " 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4346150f",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0198426b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict.get('delivery_days')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58ff3d6",
   "metadata": {},
   "source": [
    "### JsonOutputParser can stream partial JSON objects\n",
    "\n",
    "The following streaming demo will also demonstrate how the parser builds JSON objects incrementally, which is particularly beneficial for enhancing user experience in real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a829fc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Streaming JSON parsing (shows partial objects as they're built):\n",
      "------------------------------------------------------------\n",
      "Chunk: {}\n",
      "Chunk: {'gift': True}\n",
      "Chunk: {'gift': True, 'delivery_days': 2}\n",
      "Chunk: {'gift': True, 'delivery_days': 2, 'price_value': []}\n",
      "Chunk: {'gift': True, 'delivery_days': 2, 'price_value': ['']}\n",
      "Chunk: {'gift': True, 'delivery_days': 2, 'price_value': [\"It's\"]}\n",
      "Chunk: {'gift': True, 'delivery_days': 2, 'price_value': [\"It's slightly\"]}\n",
      "Chunk: {'gift': True, 'delivery_days': 2, 'price_value': [\"It's slightly more\"]}\n",
      "Chunk: {'gift': True, 'delivery_days': 2, 'price_value': [\"It's slightly more expensive\"]}\n",
      "Chunk: {'gift': True, 'delivery_days': 2, 'price_value': [\"It's slightly more expensive than\"]}\n",
      "Chunk: {'gift': True, 'delivery_days': 2, 'price_value': [\"It's slightly more expensive than the\"]}\n",
      "Chunk: {'gift': True, 'delivery_days': 2, 'price_value': [\"It's slightly more expensive than the other\"]}\n",
      "Chunk: {'gift': True, 'delivery_days': 2, 'price_value': [\"It's slightly more expensive than the other leaf\"]}\n",
      "Chunk: {'gift': True, 'delivery_days': 2, 'price_value': [\"It's slightly more expensive than the other leaf blow\"]}\n",
      "Chunk: {'gift': True, 'delivery_days': 2, 'price_value': [\"It's slightly more expensive than the other leaf blowers\"]}\n",
      "Chunk: {'gift': True, 'delivery_days': 2, 'price_value': [\"It's slightly more expensive than the other leaf blowers out\"]}\n",
      "Chunk: {'gift': True, 'delivery_days': 2, 'price_value': [\"It's slightly more expensive than the other leaf blowers out there\"]}\n",
      "Chunk: {'gift': True, 'delivery_days': 2, 'price_value': [\"It's slightly more expensive than the other leaf blowers out there,\"]}\n",
      "Chunk: {'gift': True, 'delivery_days': 2, 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but\"]}\n",
      "Chunk: {'gift': True, 'delivery_days': 2, 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I\"]}\n",
      "Chunk: {'gift': True, 'delivery_days': 2, 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think\"]}\n",
      "Chunk: {'gift': True, 'delivery_days': 2, 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's\"]}\n",
      "Chunk: {'gift': True, 'delivery_days': 2, 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth\"]}\n",
      "Chunk: {'gift': True, 'delivery_days': 2, 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it\"]}\n",
      "Chunk: {'gift': True, 'delivery_days': 2, 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for\"]}\n",
      "Chunk: {'gift': True, 'delivery_days': 2, 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the\"]}\n",
      "Chunk: {'gift': True, 'delivery_days': 2, 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra\"]}\n",
      "Chunk: {'gift': True, 'delivery_days': 2, 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features\"]}\n",
      "Chunk: {'gift': True, 'delivery_days': 2, 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]}\n",
      "------------------------------------------------------------\n",
      "‚úÖ Streaming complete!\n"
     ]
    }
   ],
   "source": [
    "# Bonus: JsonOutputParser supports streaming!\n",
    "# Let's create a chain and see how it streams partial JSON objects\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Create a simple chain: prompt -> model -> parser\n",
    "chain = prompt | chat | output_parser\n",
    "\n",
    "print(\"üîÑ Streaming JSON parsing (shows partial objects as they're built):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Stream the chain execution\n",
    "for chunk in chain.stream({\n",
    "    \"text\": customer_review,\n",
    "    \"format_instructions\": format_instructions\n",
    "}):\n",
    "    print(f\"Chunk: {chunk}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"‚úÖ Streaming complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bf3711e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìä COMPARISON: JsonOutputParser vs StructuredOutputParser\n",
      "======================================================================\n",
      "\n",
      "‚úÖ JsonOutputParser Advantages:\n",
      "   ‚Ä¢ More robust JSON parsing (handles common LLM formatting issues)\n",
      "   ‚Ä¢ Supports streaming partial JSON objects\n",
      "   ‚Ä¢ Simpler setup (no need for ResponseSchema objects)\n",
      "   ‚Ä¢ Better error handling for malformed JSON\n",
      "   ‚Ä¢ More flexible - works with any JSON structure\n",
      "\n",
      "‚ùå StructuredOutputParser Limitations:\n",
      "   ‚Ä¢ Strict JSON validation - fails on minor formatting issues\n",
      "   ‚Ä¢ No streaming support\n",
      "   ‚Ä¢ Requires ResponseSchema definitions\n",
      "   ‚Ä¢ Less forgiving of LLM output variations\n",
      "   ‚Ä¢ Immediately fails on JSON parsing errors\n",
      "\n",
      "üí° When to use each:\n",
      "   ‚Ä¢ JsonOutputParser: When you want robust, flexible JSON parsing\n",
      "   ‚Ä¢ StructuredOutputParser: When you need strict schema validation\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# üìä Comparison: JsonOutputParser vs StructuredOutputParser\n",
    "print(\"=\" * 70)\n",
    "print(\"üìä COMPARISON: JsonOutputParser vs StructuredOutputParser\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n‚úÖ JsonOutputParser Advantages:\")\n",
    "print(\"   ‚Ä¢ More robust JSON parsing (handles common LLM formatting issues)\")\n",
    "print(\"   ‚Ä¢ Supports streaming partial JSON objects\")\n",
    "print(\"   ‚Ä¢ Simpler setup (no need for ResponseSchema objects)\")\n",
    "print(\"   ‚Ä¢ Better error handling for malformed JSON\")\n",
    "print(\"   ‚Ä¢ More flexible - works with any JSON structure\")\n",
    "\n",
    "print(\"\\n‚ùå StructuredOutputParser Limitations:\")\n",
    "print(\"   ‚Ä¢ Strict JSON validation - fails on minor formatting issues\")\n",
    "print(\"   ‚Ä¢ No streaming support\")\n",
    "print(\"   ‚Ä¢ Requires ResponseSchema definitions\")\n",
    "print(\"   ‚Ä¢ Less forgiving of LLM output variations\")\n",
    "print(\"   ‚Ä¢ Immediately fails on JSON parsing errors\")\n",
    "\n",
    "print(\"\\nüí° When to use each:\")\n",
    "print(\"   ‚Ä¢ JsonOutputParser: When you want robust, flexible JSON parsing\")\n",
    "print(\"   ‚Ä¢ StructuredOutputParser: When you need strict schema validation\")\n",
    "\n",
    "print(\"=\" * 70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
